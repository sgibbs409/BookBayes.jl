

# -------------- CH 11: POLICY GRADIENT ESTIMATION --------------


export  simulate

"""
    function simulate(ğ’«::MDP, s, Ï€, d)

Generate depth-'d' sequence of state-action-reward tuples following policy `Ï€` starting from state `s`.
"""
function simulate(ğ’«::MDP, s, Ï€, d)
    Ï„ = []
    # iterate to depth d
    for i = 1:d
        # follow policy from current state
        a = Ï€(s)
        # simulate dynamics for current state-action
        sâ€², r = ğ’«.TR(s,a)
        # save trajectory step
        push!(Ï„, (s,a,r))
        # move to new state (and repeat)
        s = sâ€²
    end
    # return record of state-action-reward tuples
    return Ï„
end
