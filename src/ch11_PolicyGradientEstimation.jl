

# -------------- CH 11: POLICY GRADIENT ESTIMATION --------------


export  simulate,
        FiniteDifferenceGradient,
        gradient

"""
    function simulate(ð’«::MDP, s, Ï€, d)

Generate depth-'d' sequence of state-action-reward tuples following policy `Ï€` starting from state `s`.
"""
function simulate(ð’«::MDP, s, Ï€, d)
    Ï„ = []
    # iterate to depth d
    for i = 1:d
        # follow policy from current state
        a = Ï€(s)
        # simulate dynamics for current state-action
        sâ€², r = ð’«.TR(s,a)
        # save trajectory step
        push!(Ï„, (s,a,r))
        # move to new state (and repeat)
        s = sâ€²
    end
    # return record of state-action-reward tuples
    return Ï„
end






"""
    struct FiniteDifferenceGradient
        ð’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        Î´ # step size
    end
"""
struct FiniteDifferenceGradient
    ð’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Î´ # step size
end


@doc raw"""
    function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)

Calculate policy gradient for policy `Ï€` at parameterization `Î¸`, with respect to `Î¸`.

Returns: $\del_{\theta} U(\pi_{\theta})$
"""
function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)

    ð’«, b, d, m, Î´, Î³, n = M.ð’«, M.b, M.d, M.m, M.Î´, M.ð’«.Î³, length(Î¸)

    Î”Î¸(i) = [i == k ? Î´ : 0.0 for k in 1:n]

    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))

    U(Î¸) = mean(R(simulate(ð’«, rand(b), s->Ï€(Î¸, s), d)) for i in 1:m)

    Î”U = [U(Î¸ + Î”Î¸(i)) - U(Î¸) for i in 1:n]

    return Î”U ./ Î´
end
