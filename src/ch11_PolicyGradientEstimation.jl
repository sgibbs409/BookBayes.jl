

# -------------- CH 11: POLICY GRADIENT ESTIMATION --------------


export  simulate,
        FiniteDifferenceGradient,
        gradient,
        RegressionGradient,
        RewardToGoGradient,
        BaselineSubtractionGradient

"""
    function simulate(ğ’«::MDP, s, Ï€, d)

Generate depth-'d' sequence (trajectory `Ï„`) of state-action-reward tuples following policy `Ï€` starting from state `s`.
"""
function simulate(ğ’«::MDP, s, Ï€, d)
    Ï„ = []
    # iterate to depth d
    for i = 1:d
        # follow policy from current state
        a = Ï€(s)
        # simulate dynamics for current state-action
        sâ€², r = ğ’«.TR(s,a)
        # save trajectory step
        push!(Ï„, (s,a,r))
        # move to new state (and repeat)
        s = sâ€²
    end
    # return record of state-action-reward tuples
    return Ï„
end






"""
    struct FiniteDifferenceGradient
        ğ’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        Î´ # step size
    end
"""
struct FiniteDifferenceGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Î´ # step size
end


@doc raw"""
    function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)

Calculate policy gradient for policy `Ï€` at parameterization `Î¸`, with respect to `Î¸`.

Returns: $\nabla_{\theta} U(\pi_{\theta})$
"""
function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)

    ğ’«, b, d, m, Î´, Î³, n = M.ğ’«, M.b, M.d, M.m, M.Î´, M.ğ’«.Î³, length(Î¸)

    # helper function: generate Î´â‹…(one-hot vector length n, index i-hot)
    Î”Î¸(i) = [i == k ? Î´ : 0.0 for k in 1:n]

    # get trajectory Ï„'s discounted (depth-d) reward.
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))

    # calculate mean of discounted reward over m rollouts using policy Ï€ and parameterization Î¸, each starting from random state s âˆ¼ b.
    U(Î¸) = mean(R(simulate(ğ’«, rand(b), s->Ï€(Î¸, s), d)) for i in 1:m)

    # Calculate finite difference of U(Î¸) for each dimension, using mean of rollouts to estimate policy value function.
    Î”U = [U(Î¸ + Î”Î¸(i)) - U(Î¸) for i in 1:n]

    return Î”U ./ Î´
end


# ----- Regression Gradient ------
#
# Estimate gradient using m > 2n samples of finite differences in random directions.
"""
    struct RegressionGradient
        ğ’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        Î´ # step size
    end
"""
struct RegressionGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Î´ # step size
end

"""
    function gradient(M::RegressionGradient, Ï€, Î¸)

Estimate policy gradient centered at `Î¸` using `m` random perturbations and then using least-squares over estimated finite-differences of each.

# Example

```julia
julia> using Random

julia> using LinearAlgebra

julia> f(x) = x^2 + 1e-2*randn()

julia> m = 20

julia> Î´ = 1e-2

julia> Î”X = [Î´.*randn() for i = 1:m]

julia> x0 = 2.0

julia> Î”F = [f(x0 + Î”x) - f(x0) for Î”x in Î”X]

julia> pinv(Î”X) * Î”F       # should be around fâ€²(x==2) == 2Ã—(x==2) == 4
```
"""
function gradient(M::RegressionGradient, Ï€, Î¸)
    ğ’«, b, d, m, Î´, Î³ = M.ğ’«, M.b, M.d, M.m, M.Î´, M.ğ’«.Î³
    # m random perturbations of Î¸ from uniform distriubtion over radius Î´ hypersphere.
    Î”Î˜ = [Î´.*normalize(randn(length(Î¸)), 2) for i = 1:m]
    # calculate discounted reward from trajectory Ï„
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    # calculate discounted reward from rollout using policy Ï€ and parameterization Î¸, starting from random state s âˆ¼ b.
    U(Î¸) = R(simulate(ğ’«, rand(b), s->Ï€(Î¸,s), d))
    # finite differenes
    Î”U = [U(Î¸ + Î”Î¸) - U(Î¸) for Î”Î¸ in Î”Î˜]
    # direct solve least-squares solution of gradient using pseudoinverse
    return pinv(reduce(hcat, Î”Î˜)') * Î”U
end



"""
    struct LikelihoodRatioGradient
        ğ’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        âˆ‡logÏ€ # gradient of log likelihood
    end
"""
struct LikelihoodRatioGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€   # gradient of log likelihood
            #  ie: gradient w.r.t. Î¸ of Ï€(Î¸,a,s),
            #   where Ï€(Î¸,a,s) = probability(a | s; Î¸)
end

"""
    function gradient(M::LikelihoodRatioGradient, Ï€, Î¸)


"""
function gradient(M::LikelihoodRatioGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³

    # Ï€Î¸: stochastic/non-deterministic function of s
    Ï€Î¸(s) = Ï€(Î¸, s)

    # Calculate total discounted reward from trajectory Ï„
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))

    # âˆ‡â‚€log[pâ‚€(Ï„)] Ã— R(Ï„) = âˆ‘â‚–â‚Œâ‚(âˆ‡â‚€log[Ï€(Î¸,aâ½áµâ¾,sâ½áµâ¾)]) Ã— R(Ï„)
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)*R(Ï„)

    # monte carlo estimate of âˆ‡â‚€U(Î¸) = ğ”¼â‚œ[âˆ‡â‚€log(pâ‚€(Ï„)R(Ï„))] âˆ¼ Ï„
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end



"""
    struct RewardToGoGradient
        ğ’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        âˆ‡logÏ€ # gradient of log likelihood
    end
"""
struct RewardToGoGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end


"""
    function gradient(M::RewardToGoGradient, Ï€, Î¸)


"""
function gradient(M::RewardToGoGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)

    # see eq. 11.24: == Î³áµâ»Â¹ Ã— r_to-goâ½áµâ¾
    R(Ï„, k) = sum(r*Î³^(l-1) for (l,(s,a,r)) in zip(k:d, Ï„[k:end]))
    # see eq. 11.24
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s)*R(Ï„,k) for (k, (s,a,r)) in enumerate(Ï„))

    # monte carlo approximation to ğ”¼xpectation over Ï„
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end




"""
    struct BaselineSubtractionGradient
        ğ’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
        âˆ‡logÏ€ # gradient of log likelihood
    end
"""
struct BaselineSubtractionGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end


"""
    function gradient(M::BaselineSubtractionGradient, Ï€, Î¸)


"""
function gradient(M::BaselineSubtractionGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)

    l(a, s, k) = âˆ‡logÏ€(Î¸, a, s)*Î³^(k-1)
    # r_to-go
    R(Ï„, k) = sum(r*Î³^(j-1) for (j,(s,a,r)) in enumerate(Ï„[k:end]))

    numer(Ï„) = sum(l(a,s,k).^2*R(Ï„,k) for (k,(s,a,r)) in enumerate(Ï„))

    denom(Ï„) = sum(l(a,s,k).^2 for (k,(s,a)) in enumerate(Ï„))
    # 11.43
    base(Ï„) = numer(Ï„) ./ denom(Ï„)

    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    # 11.43
    rbase = mean(base(Ï„) for Ï„ in trajs)
    # 11.28
    âˆ‡U(Ï„) = sum(l(a,s,k).*(R(Ï„,k).-rbase) for (k,(s,a,r)) in enumerate(Ï„))
    # 11.28
    return mean(âˆ‡U(Ï„) for Ï„ in trajs)
end
