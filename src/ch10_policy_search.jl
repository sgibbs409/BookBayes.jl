


export  MonteCarloPolicyEvaluation,
        HookeJeevesPolicySearch,
        optimize,
        GeneticPolicySearch,
        CrossEntropyPolicySearch,
        optimize_dist,
        EvolutionStrategies,
        evolution_strategy_weights,
        IsotropicEvolutionStrategies


# --------------------------- POLICY SEARCH ---------------------------

# 1.  Monte Carlo Policy Evaluation
#
# Return the mean discounted reward of m random trajectories,
# each calculated by first sampling the initial state distribution,
# and then executing rollout from that state, using policy Ï€.

"""
    struct MonteCarloPolicyEvaluation
        ð’« # problem
        b # initial state distribution
        d # depth
        m # number of samples
    end
"""
struct MonteCarloPolicyEvaluation
    ð’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
end

@doc raw"""
    function (U::MonteCarloPolicyEvaluation)(Ï€)

Return the mean discounted reward of m random trajectories, each calculated by first sampling the initial state distribution, and then executing rollout from that state, using policy Ï€.

$U(\pi) = E_{\tau}[R(\tau)] = \sum_{\tau} p(\tau)R(\tau) \approx \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)})$

Returns: UÌƒ(Ï€)

Complexity: ð’ª(m Ã— d)
"""
function (U::MonteCarloPolicyEvaluation)(Ï€)
    R(Ï€) = rollout(U.ð’«, rand(U.b), Ï€, U.d) # ð’ª(d)
    return mean(R(Ï€) for i = 1:U.m)
end


"""
    (U::MonteCarloPolicyEvaluation)(Ï€, Î¸) = U(s->Ï€(Î¸, s))

Parameterized version where policy adjusted by parameter vector `Î¸`.

Returns: UÌƒ(Ï€â‚Î¸â‚Ž)

Complexity: ð’ª(m Ã— d)
"""
(U::MonteCarloPolicyEvaluation)(Ï€, Î¸) = U(s->Ï€(Î¸, s))



# 2.  Local Search (Hooke-Jeeves)
#
# Start w/initial parameterization and move from neighbor to neighbor
#   until convergence.
# Î¸: n-dimensional vector
# Takes step of Â±Î± in each coordinate direction from Î¸.  If improvement found, moves to best neighbor.  Else stepsize reduced to Î± *= c for some c < 1.0.  Continue until Î± < Ïµ for some Ïµ > 0.

"""
    struct HookeJeevesPolicySearch
        Î¸ # initial parameterization
        Î± # step size
        c # step size reduction factor
        Îµ # termination step size
    end
"""
struct HookeJeevesPolicySearch
    Î¸ # initial parameterization
    Î± # step size
    c # step size reduction factor
    Îµ # termination step size
end

"""
    function optimize(M::HookeJeevesPolicySearch, Ï€, U)
"""
function optimize(M::HookeJeevesPolicySearch, Ï€, U)
    Î¸, Î¸â€², Î±, c, Îµ = copy(M.Î¸), similar(M.Î¸), M.Î±, M.c, M.Îµ
    u, n = U(Ï€, Î¸), length(Î¸)
    while Î± > Îµ
        copyto!(Î¸â€², Î¸)
        best = (i=0, sgn=0, u=u)
        for i in 1:n
            for sgn in (-1,1)
                Î¸â€²[i] = Î¸[i] + sgn*Î±
                uâ€² = U(Ï€, Î¸â€²)
                if uâ€² > best.u
                    best = (i=i, sgn=sgn, u=uâ€²)
                end
            end
            Î¸â€²[i] = Î¸[i]
        end
        if best.i != 0
            Î¸[best.i] += best.sgn*Î±
            u = best.u
        else
            Î± *= c
        end
    end
    return Î¸
end


# 3.  Genetic Policy Search

"""
    struct GeneticPolicySearch
        Î¸s # initial population
        Ïƒ # initial standard devidation
        m_elite # number of elite samples
        k_max # number of iterations
    end
"""
struct GeneticPolicySearch
    Î¸s # initial population
    Ïƒ # initial standard devidation
    m_elite # number of elite samples
    k_max # number of iterations
end

"""
    function optimize(M::GeneticPolicySearch, Ï€, U)
"""
function optimize(M::GeneticPolicySearch, Ï€, U)
    Î¸s, Ïƒ = M.Î¸s, M.Ïƒ
    n, m = length(first(Î¸s)), length(Î¸s)
    for k in 1:M.k_max
        us = [U(Ï€, Î¸) for Î¸ in Î¸s]
        sp = sortperm(us, rev=true)
        Î¸_best = Î¸s[sp[1]]
        rand_elite() = Î¸s[sp[rand(1:M.m_elite)]]
        Î¸s = [rand_elite() + Ïƒ.*randn(n) for i in 1:(m-1)]
        push!(Î¸s, Î¸_best)
    end
    return last(Î¸s)
end


# 4.  Cross Entropy Method

"""
    struct CrossEntropyPolicySearch
        p # initial distribution
        m # number of samples
        m_elite # number of elite samples
        k_max # number of iterations
    end
"""
struct CrossEntropyPolicySearch
    p # initial distribution
    m # number of samples
    m_elite # number of elite samples
    k_max # number of iterations
end

"""
    function optimize_dist(M::CrossEntropyPolicySearch, Ï€, U)
"""
function optimize_dist(M::CrossEntropyPolicySearch, Ï€, U)
    p, m, m_elite, k_max = M.p, M.m, M.m_elite, M.k_max
    for k in 1:k_max
        Î¸s = rand(p, m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        Î¸_elite = Î¸s[:,sortperm(us)[(m-m_elite+1):m]]
        p = Distributions.fit(typeof(p), Î¸_elite)
    end
    return p
end

"""
    function optimize(M, Ï€, U)
"""
function optimize(M, Ï€, U)
    return Distributions.mode(optimize_dist(M, Ï€, U))
end



# 5.  Evolutionary Strategies
#
# Update a search distribution parameterized by a vector Ïˆ at
#   each iteration by taking a step in the direciton of the gradient of
#   the Expected Policy Utility function.
"""
    struct EvolutionStrategies
        D # distribution constructor
        Ïˆ # initial distribution parameterization
        âˆ‡logp # log search likelihood gradient
        m # number of samples
        Î± # step factor
        k_max # number of iterations
    end
"""
struct EvolutionStrategies
    D # distribution constructor
    Ïˆ # initial distribution parameterization
    âˆ‡logp # log search likelihood gradient
    m # number of samples
    Î± # step factor
    k_max # number of iterations
end

"""
    function evolution_strategy_weights(m)
"""
function evolution_strategy_weights(m)
    ws = [max(0, log(m/2+1) - log(i)) for i in 1:m]
    ws ./= sum(ws)
    ws .-= 1/m

    return ws
end

"""
    function optimize_dist(M::EvolutionStrategies, Ï€, U)
"""
function optimize_dist(M::EvolutionStrategies, Ï€, U)
    D, Ïˆ, m, âˆ‡logp, Î± = M.D, M.Ïˆ, M.m, M.âˆ‡logp, M.Î±
    ws = evolution_strategy_weights(m)
    for k in 1:M.k_max
        Î¸s = rand(D(Ïˆ), m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*âˆ‡logp(Ïˆ, Î¸s[:,i]) for (w,i) in zip(ws,sp))
        Ïˆ += Î±.*âˆ‡
    end
    return D(Ïˆ)
end


# 6.  Isotropic Evolutionary Strategies
#
# Same as Evolutionary Strategies, but use mirroring of samples to reduce
#   gradient variation.
"""
    struct IsotropicEvolutionStrategies
        Ïˆ # initial mean
        Ïƒ # initial standard devidation
        m # number of samples
        Î± # step factor
        k_max # number of iterations
    end
"""
struct IsotropicEvolutionStrategies
    Ïˆ # initial mean
    Ïƒ # initial standard devidation
    m # number of samples
    Î± # step factor
    k_max # number of iterations
end

"""
    function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
"""
function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
    Ïˆ, Ïƒ, m, Î±, k_max = M.Ïˆ, M.Ïƒ, M.m, M.Î±, M.k_max
    n = length(Ïˆ)
    ws = evolution_strategy_weights(2*div(m,2))
    for k in 1:k_max
        Îµs = [randn(n) for i in 1:div(m,2)]
        append!(Îµs, -Îµs) # weight mirroring
        us = [U(Ï€, Ïˆ + Ïƒ.*Îµ) for Îµ in Îµs]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*Îµs[i] for (w,i) in zip(ws,sp)) / Ïƒ
        Ïˆ += Î±.*âˆ‡
    end
    return MvNormal(Ïˆ, Ïƒ)
end
