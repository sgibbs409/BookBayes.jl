



export  MDF,
        lookahead,
        iterative_policy_evaluation,
        policy_evaluation,
        ValueFunctionPolicy,
        greedy,
        PolicyIteration,
        solve,
        backup,
        ValueIteration,
        GaussSeidelValueIteration,
        LinearProgramFormulation,
        tensorform,
        LinearQuadraticProblem


"""
    struct MDP
        Î³ # discount factor
        ğ’® # state space
        ğ’œ # action space
        T # transition function
        R # reward function
        TR # sample transition and reward
    end
"""
struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end

"""
    function lookahead(ğ’«::MDP, U, s, a)

Calculate state-action value function Q(s,a) from U and valid state transition distribution T:

`Returns`
    Q(s,a) = R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)

"""
function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

"""
    function lookahead(ğ’«::MDP, U::Vector, s, a)

Calculate state-action value function Q(s,a) from U and valid state transition distribution T:

    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))

"""
function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end



"""
    function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)

 Approximate policy evaluation for policy `Ï€`, using k_max iterations of 1-step lookahead, starting from U(s) = 0.
"""
function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end



"""
    function policy_evaluation(ğ’«::MDP, Ï€)

Exact policy evaluation using matrix-based Moore-Penrose pseudoinverse equation.
Requires O(|ğ’®|Â³) time.

`Returns:` [ U(s) âˆ€ s âˆˆ {ğ’®} ]
"""
function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³

    # R(s, Ï€(s)) âˆ€ s âˆˆ {ğ’®}
    Râ€² = [R(s, Ï€(s)) for s in ğ’®] # nx1 = |ğ’®|x1
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®] # nxm = |ğ’®|x|ğ’®|
    return (I - Î³*Tâ€²)\Râ€²
end



"""
    struct ValueFunctionPolicy
        ğ’« # problem
        U # utility function
    end
"""
struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

"""
    function greedy(ğ’«::MDP, U, s)

Find the greedy action and its expected utility starting from state `s` and using estimated optimal (greedy) state value function `U`.  Calculates estimate as maximum over action space of 1-step lookahead:

    u, a = _findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
"""
function greedy(ğ’«::MDP, U, s)
    u, a = _findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

"""
    function (Ï€::ValueFunctionPolicy)(s)

Greedy policy function mapping states to greedy action.
"""
function (Ï€::ValueFunctionPolicy)(s)
    return greedy(Ï€.ğ’«, Ï€.U, s).a
end


# ---------------- Policy Iteration ----------------

"""
    struct PolicyIteration
        Ï€ # initial policy
        k_max # maximum number of iterations
    end
"""
struct PolicyIteration
    Ï€ # initial policy
    k_max # maximum number of iterations
end

"""
    function solve(M::PolicyIteration, ğ’«::MDP)

Iteratively improves an initial policy `Ï€` to obtain an optimal policy for an MDP `ğ’«` with discrete state and action spaces.  Returns improved policy.
"""
function solve(M::PolicyIteration, ğ’«::MDP)
    Ï€, ğ’® = M.Ï€, ğ’«.ğ’®
    converged = false

    for k = 1:M.k_max
        # exact solution for U_Ï€(s)
        U = policy_evaluation(ğ’«, Ï€)
        # Use new utility function for finding greedy action.
        Ï€â€² = ValueFunctionPolicy(ğ’«, U)
        # same action given for every state, => convergence (current Ï€ is best)
        if all(Ï€(s) == Ï€â€²(s) for s in ğ’®)
            break
        end
        # Not converged: update policy with new greedy policy.
        Ï€ = Ï€â€²
    end
    return Ï€
end



# -------------------------- Value Iteration --------------------------

"""
    function backup(ğ’«::MDP, U, s)

Calculate maxâ‚ Q(s,a).
Complexity: O(|ğ’®|Ã—|ğ’œ|).

    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
"""
function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end


"""
    struct ValueIteration
        k_max # maximum number of iterations
    end
"""
struct ValueIteration
    k_max # maximum number of iterations
end

"""
    function solve(M::ValueIteration, ğ’«::MDP)

Iteratively improves a value function `U` to obtain an optimal policy for an MDP `ğ’«` with discrete state and action spaces.  Terminates after `k_max` iterations.

Order O(k_max Ã— |ğ’®| Ã— |ğ’œ| Ã— |ğ’®|)
# Returns

Optimal policy `Ï€`
"""
function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end

#
# @doc raw"""
#     function solve(M::ValueIteration, ğ’«::MDP, Î´::Float64)
#
# Iteratively improves a value function `U` to obtain an optimal policy for an MDP `ğ’«` with discrete state and action spaces.  Terminates after Bellman residual falls below threshold Î´:
#
# $\|\|U\_{k+1} - U\_k\|\|\_{\infty} < \delta$
#
# # Returns
#
# Optimal policy `Ï€`
# """
# function solve(M::ValueIteration, ğ’«::MDP, Î´::Float64)
#     U = [0.0 for s in ğ’«.ğ’®]
#     for k = 1:M.k_max
#         U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
#     end
#     return ValueFunctionPolicy(ğ’«, U)
# end



# ----------------- Asyncronous Value Iteration --------------------
#
# Similar to Value Iteration, but only subset of states updated every iteration.
#
# Common asyncronous value iteration method: Gauss-Seidel.
#   Sweeps through an ordering of states and applies Bellman update in place.
"""
    struct GaussSeidelValueIteration
        k_max # maximum number of iterations
    end
"""
struct GaussSeidelValueIteration
    k_max # maximum number of iterations
end

"""
    function solve(M::GaussSeidelValueIteration, ğ’«::MDP)

Same as Value Iteration but update U one state at a time.  Convergence speed depends on ordering of states in `ğ’®`.
"""
function solve(M::GaussSeidelValueIteration, ğ’«::MDP)
    ğ’®, ğ’œ, T, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k = 1:M.k_max
        for (i, s) in enumerate(ğ’®)
            u = backup(ğ’«, U, s)
            U[i] = u
        end
    end
    return ValueFunctionPolicy(ğ’«, U)
end


# ------------- Linear Programming --------------

"""
    struct LinearProgramFormulation end
"""
struct LinearProgramFormulation end

"""
    function tensorform(ğ’«::MDP)

Convert an MDP into its tensor form, where the states and actions consist of integer indicies, the reward function is a matrix, and the transition function is a three-dimensional tensor.
"""
function tensorform(ğ’«::MDP)
    ğ’®, ğ’œ, R, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s,a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    return ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end


"""
    solve(ğ’«::MDP) = solve(LinearProgramFormulation(), ğ’«)

Default MDP solver is the LinearProgramFormulation solver.
"""
solve(ğ’«::MDP) = solve(LinearProgramFormulation(), ğ’«)


"""
    function solve(M::LinearProgramFormulation, ğ’«::MDP)
"""
function solve(M::LinearProgramFormulation, ğ’«::MDP)
    ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ğ’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s=ğ’®,a=ğ’œ], U[s] â‰¥ R[s,a] + ğ’«.Î³*T[s,a,:]â‹…U)
    optimize!(model)
    return ValueFunctionPolicy(ğ’«, value.(U))
end


# ---------- Linear Quadratic Problem -----------


"""
    struct LinearQuadraticProblem
        Ts # transition matrix with respect to state
        Ta # transition matrix with respect to action
        Rs # reward matrix with respect to state (negative semidefinite)
        Ra # reward matrix with respect to action (negative definite)
        h_max # horizon
    end
"""
struct LinearQuadraticProblem
    Ts # transition matrix with respect to state
    Ta # transition matrix with respect to action
    Rs # reward matrix with respect to state (negative semidefinite)
    Ra # reward matrix with respect to action (negative definite)
    h_max # horizon
end

"""
    function solve(ğ’«::LinearQuadraticProblem)
"""
function solve(ğ’«::LinearQuadraticProblem)
    Ts, Ta, Rs, Ra, h_max = ğ’«.Ts, ğ’«.Ta, ğ’«.Rs, ğ’«.Ra, ğ’«.h_max
    V = zeros(size(Rs))
    Ï€s = Any[s -> zeros(size(Ta, 2))]
    for h in 2:h_max
        V = Ts'*(V - V*Ta*((Ta'*V*Ta + Ra) \ Ta'*V))*Ts + Rs
        L = -(Ta'*V*Ta + Ra) \ Ta' * V * Ts
        push!(Ï€s, s -> L*s)
    end
    return Ï€s
end
