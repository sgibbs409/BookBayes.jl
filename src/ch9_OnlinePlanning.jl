
# ------------------- Ch. 9: Online Planning --------------------

# Reachable state space usually much smaller than than full state space.
#
# Algorithms to compute approximately optimal policies starting
#   from the current state.


export  RolloutLookahead,
        randstep,
        rollout,
        ForwardSearch,
        forward_search,
        BranchAndBound,
        branch_and_bound,
        SparseSampling,
        sparse_sampling,
        MonteCarloTreeSearch,
        simulate!,
        bonus,
        explore,
        HeuristicSearch,
        LabeledHeuristicSearch,
        expand,
        label!




# -------- Rollout with Lookahead ---------

"""
    struct RolloutLookahead
        ğ’« # problem
        Ï€ # rollout policy
        d # depth
    end
"""
struct RolloutLookahead
    ğ’« # problem
    Ï€ # rollout policy
    d # depth
end

"""
    randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)
"""
randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)

"""
    function rollout(ğ’«, s, Ï€, d)

Complexity: ğ’ª(d)
"""
function rollout(ğ’«, s, Ï€, d)
    if d â‰¤ 0
        return 0.0
    end
    a = Ï€(s)
    sâ€², r = randstep(ğ’«, s, a)
    return r + ğ’«.Î³*rollout(ğ’«, sâ€², Ï€, d-1)
end


"""
    function (Ï€::RolloutLookahead)(s)

Policy function: similar to ValueFunctionPolicy but using a single call to rollout for value function.

Complexity: O(|ğ’œ|Ã—|ğ’®| Ã— d)

Consider variation that uses average of m rollouts to calculate U(s) (vs just 1 with this version).
"""
function (Ï€::RolloutLookahead)(s)
    U(s) = rollout(Ï€.ğ’«, s, Ï€.Ï€, Ï€.d)
    return greedy(Ï€.ğ’«, U, s).a
end


# -------- Forward Search ---------
"""
    struct ForwardSearch
        ğ’« # problem
        d # depth
        U # value function at depth d
    end
"""
struct ForwardSearch
    ğ’« # problem
    d # depth
    U # value function at depth d
end

"""
    function forward_search(ğ’«, s, d, U)

Determine optimal action (and its value) to take from state `s` by expanding all possible transitions up to a depth `d` using depth-first search. `U(s)` used to evaluate the terminal (depth 0) value function.

Complexity: ğ’ª( (|ğ’œ|Ã—|ğ’®|)áµˆ )
"""
function forward_search(ğ’«, s, d, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end

    best = (a=nothing, u=-Inf)
    Uâ€²(s) = forward_search(ğ’«, s, d-1, U).u

    for a in ğ’«.ğ’œ
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end

    return best
end

(Ï€::ForwardSearch)(s) = forward_search(Ï€.ğ’«, s, Ï€.d, Ï€.U).a

# --------- Branch and Bound ---------

"""
    struct BranchAndBound
        ğ’« # problem
        d # depth
        Ulo # lower bound on value function at depth d Qhi # upper bound on action value function
    end
"""
struct BranchAndBound
    ğ’« # problem
    d # depth
    Ulo # lower bound on value function at depth d
    Qhi # upper bound on action value function
end


"""
    function branch_and_bound(ğ’«, s, d, Ulo, Qhi)
"""
function branch_and_bound(ğ’«, s, d, Ulo, Qhi)
    if d â‰¤ 0
        return (a=nothing, u=Ulo(s))
    end

    Uâ€²(s) = branch_and_bound(ğ’«, s, d-1, Ulo, Qhi).u
    best = (a=nothing, u=-Inf)
    for a in sort(ğ’«.ğ’œ, by=a->Qhi(s,a), rev=true)
        if Qhi(s, a) < best.u
            return best # safe to prune
        end
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end

(Ï€::BranchAndBound)(s) = branch_and_bound(Ï€.ğ’«, s, Ï€.d, Ï€.Ulo, Ï€.Qhi).a




# --------- Sparse Sampling ---------

"""
    struct SparseSampling
        ğ’« # problem
        d # depth
        m # number of samples
        U # value function at depth d
    end
"""
struct SparseSampling
    ğ’« # problem
    d # depth
    m # number of samples
    U # value function at depth d
end


"""
    function sparse_sampling(ğ’«, s, d, m, U)

Complexity: ğ’ª( (|ğ’œ| Ã— m)áµˆ )
"""
function sparse_sampling(ğ’«, s, d, m, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end

    best = (a=nothing, u=-Inf)

    for a in ğ’«.ğ’œ
        u = 0.0
        for i in 1:m
            sâ€², r = randstep(ğ’«, s, a)
            aâ€², uâ€² = sparse_sampling(ğ’«, sâ€², d-1, m, U)
            u += (r + ğ’«.Î³*uâ€²) / m  # update mean
        end

        if u > best.u
            best = (a=a, u=u)
        end
    end

    return best
end

(Ï€::SparseSampling)(s) = sparse_sampling(Ï€.ğ’«, s, Ï€.d, Ï€.m, Ï€.U).a




# --------- Monte Carlo Tree Search ---------

"""
    struct MonteCarloTreeSearch
        ğ’« # problem
        N # visit counts
        Q # action value estimates
        d # depth
        m # number of simulations
        c # exploration constant
        Ï€ # rollout policy
    end
"""
struct MonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    Ï€ # rollout policy
end


"""
    function (Ï€::MonteCarloTreeSearch)(s)

Estimate optimal next action by first simulating m random trajectories.  Each simulation improves estimated action-value function.  The returned action is the optimal action over the estimated action-value function from the given state `s`.

Complexity: ğ’ª(|ğ’œ| Ã— d Ã— m)
"""
function (Ï€::MonteCarloTreeSearch)(s)
    # simulate m times (m trajectories)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    # Return action with highest action-value
    return _argmax(a->Ï€.Q[(s,a)], Ï€.ğ’«.ğ’œ)
end


"""
    function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)

Execute exploration-bonus policy for 1 random trajectory, updating system  counters, and return sample trajectory's value.

Complexity: `ğ’ª(|ğ’œ| Ã— d)`
"""
function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return 0.0
    end

    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³

    # if never visited state s, init visit count and action-value estimate.
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        # first time to state s: return sample trajectory value as utility est.
        return rollout(ğ’«, s, Ï€.Ï€, d)
    end

    # find next action to try
    a = explore(Ï€, s) # ğ’ª(|ğ’œ|)
    # simulate 1 step
    sâ€², r = TR(s,a)
    # recurse remaining stochastic trajectory
    q = r + Î³*simulate!(Ï€, sâ€², d-1)
    # update visit count
    N[(s,a)] += 1
    # update running average of action-value
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]

    return q
end


"""
    bonus(Nsa, Ns)

Monte Carlo exploration bonus term helper function.

Complexity: ğ’ª(1)
"""
bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)


"""
    function explore(Ï€::MonteCarloTreeSearch, s)

Use monte carlo exploration heuristic to find next action to simulate.  Balances need to explore state-action space with
Complexity: ğ’ª(|ğ’œ|)
"""
function explore(Ï€::MonteCarloTreeSearch, s)

    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c

    Ns = sum(N[(s,a)] for a in ğ’œ)

    return _argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end


# --------- Heuristic Search ---------

# Use m simulations of a greedy policy with respect to value function U from state s.  U is initialized to an upperbound UÌ„ (referred to as a heuristic).  Updates U with each lookahead step during simulation.  After simulations and value function estimate U improvement, return greedy action.
"""
    struct HeuristicSearch
        ğ’« # problem
        Uhi # upper bound on value function
        d # depth
        m # number of simulations
    end
"""
struct HeuristicSearch
    ğ’« # problem
    Uhi # upper bound on value function
    d # depth
    m # number of simulations
end


"""
    function simulate!(Ï€::HeuristicSearch, U, s)

Simulate depth-d trajectory following greedy policy and updating U along the way.

Complexity: ğ’ª(d Ã— |ğ’œ|Ã—|ğ’®|)
"""
function simulate!(Ï€::HeuristicSearch, U, s)
    ğ’«, d = Ï€.ğ’«, Ï€.d
    for d in 1:d                # ğ’ª( d Ã— ... (|ğ’œ|Ã—|ğ’®|))
        a, u = greedy(ğ’«, U, s) # ğ’ª(|ğ’œ|Ã—|ğ’®|)
        U[s] = u
        s = rand(ğ’«.T(s, a))
    end
end


"""
    function (Ï€::HeuristicSearch)(s)

Guranteed to converge to optimal value function iff `Uhi` is indeed an upperbound on `U`.

Complexity: ğ’ª(m Ã— d Ã— |ğ’®|Ã—|ğ’œ|)
"""
function (Ï€::HeuristicSearch)(s)
    # initialize U with upper bound
    U = [Ï€.Uhi(s) for s in Ï€.ğ’«.ğ’®]
    # run m random trajectory simulations to improve estimate on U.
    for i in 1:m
        simulate!(Ï€, U, s)
    end
    # Return greedy action using improved U.
    return greedy(Ï€.ğ’«, U, s).a
end

# --------- Labled Heuristic Search ---------


"""
    struct LabeledHeuristicSearch
        ğ’« # problem
        Uhi # upper bound on value function
        d # depth
        Î´ # gap threshold
    end
"""
struct LabeledHeuristicSearch
    ğ’« # problem
    Uhi # upper bound on value function
    d # depth
    Î´ # gap threshold
end


"""
    function (Ï€::LabeledHeuristicSearch)(s)
"""
function (Ï€::LabeledHeuristicSearch)(s)
    U, solved = [Ï€.Uhi(s) for s in ğ’«.ğ’®], Set()
    while s âˆ‰ solved
        simulate!(Ï€, U, solved, s)
    end
    return greedy(Ï€.ğ’«, U, s).a
end


"""
    function simulate!(Ï€::LabeledHeuristicSearch, U, solved, s)
"""
function simulate!(Ï€::LabeledHeuristicSearch, U, solved, s)
    visited = []
    for d in 1:Ï€.d
        if s âˆˆ solved
            break
        end

        push!(visited, s)
        a, u = greedy(Ï€.ğ’«, U, s)
        U[s] = u
        s = rand(Ï€.ğ’«.T(s, a))
    end

    while !isempty(visited)
        if label!(Ï€, U, solved, pop!(visited))
            break
        end
    end
end


"""
    function expand(Ï€::LabeledHeuristicSearch, U, solved, s)
"""
function expand(Ï€::LabeledHeuristicSearch, U, solved, s)
    ğ’«, Î´ = Ï€.ğ’«, Ï€.Î´
    ğ’®, ğ’œ, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T

    found, toexpand, envelope = false, Set(s), []
    while !isempty(toexpand)
        s = pop!(toexpand)
        push!(envelope, s)
        a, u = greedy(ğ’«, U, s)
        if abs(U[s] - u) > Î´
            found = true
        else
            for sâ€² in ğ’®
                if T(s,a,sâ€²) > 0 && sâ€² âˆ‰ (solved âˆª envelope)
                    push!(toexpand, sâ€²)
                end
            end
        end
    end

    return (found, envelope)
end


"""
    function label!(Ï€::LabeledHeuristicSearch, U, solved, s)
"""
function label!(Ï€::LabeledHeuristicSearch, U, solved, s)
    if s âˆˆ solved
        return false
    end
    found, envelope = expand(Ï€, U, solved, s)
    if found
        for s âˆˆ reverse(envelope)
            U[s] = greedy(Ï€.ğ’«, U, s).u
        end
    else
        union!(solved, envelope)
    end

    return found
end
